{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a6266bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langgraph\n",
      "  Downloading langgraph-0.4.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: langchain-core>=0.1 in c:\\users\\cheekish\\desktop\\gen-ai\\.venv\\lib\\site-packages (from langgraph) (0.3.56)\n",
      "Collecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph)\n",
      "  Downloading langgraph_checkpoint-2.0.25-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting langgraph-prebuilt>=0.1.8 (from langgraph)\n",
      "  Downloading langgraph_prebuilt-0.1.8-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting langgraph-sdk>=0.1.42 (from langgraph)\n",
      "  Downloading langgraph_sdk-0.1.63-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting xxhash<4.0.0,>=3.5.0 (from langgraph)\n",
      "  Using cached xxhash-3.5.0-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
      "Collecting ormsgpack<2.0.0,>=1.8.0 (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph)\n",
      "  Using cached ormsgpack-1.9.1-cp311-cp311-win_amd64.whl.metadata (44 kB)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\cheekish\\desktop\\gen-ai\\.venv\\lib\\site-packages (from langchain-core>=0.1->langgraph) (0.3.38)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\cheekish\\desktop\\gen-ai\\.venv\\lib\\site-packages (from langchain-core>=0.1->langgraph) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\cheekish\\desktop\\gen-ai\\.venv\\lib\\site-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\cheekish\\desktop\\gen-ai\\.venv\\lib\\site-packages (from langchain-core>=0.1->langgraph) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\cheekish\\desktop\\gen-ai\\.venv\\lib\\site-packages (from langchain-core>=0.1->langgraph) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\cheekish\\desktop\\gen-ai\\.venv\\lib\\site-packages (from langchain-core>=0.1->langgraph) (4.13.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in c:\\users\\cheekish\\desktop\\gen-ai\\.venv\\lib\\site-packages (from langchain-core>=0.1->langgraph) (2.11.4)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\cheekish\\desktop\\gen-ai\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.1->langgraph) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\cheekish\\desktop\\gen-ai\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core>=0.1->langgraph) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\cheekish\\desktop\\gen-ai\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core>=0.1->langgraph) (3.10.18)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\cheekish\\desktop\\gen-ai\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core>=0.1->langgraph) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\cheekish\\desktop\\gen-ai\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core>=0.1->langgraph) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\cheekish\\desktop\\gen-ai\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core>=0.1->langgraph) (0.23.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\cheekish\\desktop\\gen-ai\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core>=0.1->langgraph) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\cheekish\\desktop\\gen-ai\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core>=0.1->langgraph) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\cheekish\\desktop\\gen-ai\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core>=0.1->langgraph) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\cheekish\\desktop\\gen-ai\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core>=0.1->langgraph) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\cheekish\\desktop\\gen-ai\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core>=0.1->langgraph) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\cheekish\\desktop\\gen-ai\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core>=0.1->langgraph) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\cheekish\\desktop\\gen-ai\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core>=0.1->langgraph) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\cheekish\\desktop\\gen-ai\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core>=0.1->langgraph) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\cheekish\\desktop\\gen-ai\\.venv\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core>=0.1->langgraph) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\cheekish\\desktop\\gen-ai\\.venv\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core>=0.1->langgraph) (2.4.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\cheekish\\desktop\\gen-ai\\.venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core>=0.1->langgraph) (1.3.1)\n",
      "Downloading langgraph-0.4.0-py3-none-any.whl (148 kB)\n",
      "Downloading langgraph_checkpoint-2.0.25-py3-none-any.whl (42 kB)\n",
      "Using cached ormsgpack-1.9.1-cp311-cp311-win_amd64.whl (125 kB)\n",
      "Using cached xxhash-3.5.0-cp311-cp311-win_amd64.whl (30 kB)\n",
      "Downloading langgraph_prebuilt-0.1.8-py3-none-any.whl (25 kB)\n",
      "Downloading langgraph_sdk-0.1.63-py3-none-any.whl (47 kB)\n",
      "Installing collected packages: xxhash, ormsgpack, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph\n",
      "\n",
      "   -------------------- ------------------- 3/6 [langgraph-checkpoint]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   ---------------------------------------- 6/6 [langgraph]\n",
      "\n",
      "Successfully installed langgraph-0.4.0 langgraph-checkpoint-2.0.25 langgraph-prebuilt-0.1.8 langgraph-sdk-0.1.63 ormsgpack-1.9.1 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "! pip install langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9111e047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91985d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"https://genaitcgazuregpt.openai.azure.com\"  # Extracted from the URL\n",
      "\"6202aa112d964a35aa3b08fe5d5f2700\"  # Replace with your Azure API Key\n",
      "\"gpt-4o\"  # Extracted from the URL\n"
     ]
    }
   ],
   "source": [
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "gpt4_deployment_name = os.getenv(\"AZURE_DEPLOYMENT_NAME\")\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "\n",
    "print(azure_endpoint)\n",
    "print(azure_api_key)\n",
    "\n",
    "print(gpt4_deployment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d5b07a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_api_key = \"6202aa112d964a35aa3b08fe5d5f2700\"  # Replace with your Azure API Key\n",
    "OPENAI_API_VERSION = \"2024-02-15-preview\"\n",
    "gpt4_deployment_name = \"gpt-4o\"  # Extracted from the URL\n",
    "azure_endpoint = \"https://genaitcgazuregpt.openai.azure.com\"  # Extracted from the URL\n",
    "AZURE_MODEL_VERSION = \"2024-02-15-preview\"  # Likely same as API version unless specified otherwise\n",
    "USE_AZURE_OPENAI = \"yes\"\n",
    "RESPONSE_ENGINE=\"azure_openai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aac3e37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "llm = AzureChatOpenAI(\n",
    "        azure_endpoint=azure_endpoint,\n",
    "        api_key=azure_api_key,\n",
    "        deployment_name=gpt4_deployment_name,\n",
    "        api_version=\"2024-08-01-preview\",\n",
    "        temperature=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d536aba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://genaitcgazuregpt.openai.azure.com\n"
     ]
    }
   ],
   "source": [
    "print(azure_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b53bf51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tools = [TavilySearchResults(max_results=3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fc269940",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# Choose the LLM that will drive the agent\n",
    "prompt = \"You are a helpful assistant.\"\n",
    "agent_executor = create_react_agent(llm, tools, prompt=prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7b7808dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='who is the winnner of the us open', additional_kwargs={}, response_metadata={}, id='0df95afd-3f35-4d39-ae7b-1105ce60f734'),\n",
       "  AIMessage(content='Could you specify which US Open you are referring to? There are multiple US Open tournaments, such as the US Open Tennis Championships and the US Open Golf Championship.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 95, 'total_tokens': 129, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_ee1d74bde0', 'id': 'chatcmpl-BRxVGNGtlG3bJd3MG5Io3npR5dzJb', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-16c70328-7e0e-42d2-b611-dbfd26517f6d-0', usage_metadata={'input_tokens': 95, 'output_tokens': 34, 'total_tokens': 129, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"messages\": [(\"user\", \"who is the winnner of the us open\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1c4b8537",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import operator\n",
    "from typing import Annotated, List, Tuple\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class PlanExecute(TypedDict):\n",
    "    input: str\n",
    "    plan: List[str]\n",
    "    past_steps: Annotated[List[Tuple], operator.add]\n",
    "    response: str\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c3f6e601",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"Plan to follow in future\"\"\"\n",
    "\n",
    "    steps: List[str] = Field(\n",
    "        description=\"different steps to follow, should be in sorted order\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "48d7aa56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Plan(steps=['Identify the current winner of the Australia Open. This involves checking the latest results from the Australia Open tournament, which is held annually in January.', 'Once the winner is identified, research their personal background to find their hometown. This can be done by looking at reliable sources such as sports news websites, official player profiles, or interviews.', 'The hometown of the current Australia Open winner is the final answer.'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "planner_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"For the given objective, come up with a simple step by step plan. \\\n",
    "This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\n",
    "The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\"\"\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "planner = planner_prompt | llm.with_structured_output(Plan)\n",
    "planner.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            (\"user\", \"what is the hometown of the current Australia open winner?\")\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "511f05c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Plan(steps=['Identify the current winner of the Australia Open for the year 2023.', 'Research the hometown of the identified winner.', 'Provide the hometown as the final answer.'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "planner.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            (\"user\", \"what is the hometown of the current Australia open winner?\")\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "818e7be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "\n",
    "class Response(BaseModel):\n",
    "    \"\"\"Response to user.\"\"\"\n",
    "\n",
    "    response: str\n",
    "\n",
    "\n",
    "class Act(BaseModel):\n",
    "    \"\"\"Action to perform.\"\"\"\n",
    "\n",
    "    action: Union[Response, Plan] = Field(\n",
    "        description=\"Action to perform. If you want to respond to user, use Response. \"\n",
    "        \"If you need to further use tools to get the answer, use Plan.\"\n",
    "    )\n",
    "\n",
    "\n",
    "replanner_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"For the given objective, come up with a simple step by step plan. \\\n",
    "This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\n",
    "The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n",
    "\n",
    "Your objective was this:\n",
    "{input}\n",
    "\n",
    "Your original plan was this:\n",
    "{plan}\n",
    "\n",
    "You have currently done the follow steps:\n",
    "{past_steps}\n",
    "\n",
    "Update your plan accordingly. If no more steps are needed and you can return to the user, then respond with that. Otherwise, fill out the plan. Only add steps to the plan that still NEED to be done. Do not return previously done steps as part of the plan.\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "replanner = replanner_prompt | ChatOpenAI(\n",
    "    model=\"gpt-4o\", temperature=0\n",
    ").with_structured_output(Act)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "64f0fa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langgraph.graph import END\n",
    "\n",
    "\n",
    "async def execute_step(state: PlanExecute):\n",
    "    plan = state[\"plan\"]\n",
    "    plan_str = \"\\n\".join(f\"{i+1}. {step}\" for i, step in enumerate(plan))\n",
    "    task = plan[0]\n",
    "    task_formatted = f\"\"\"For the following plan:\n",
    "{plan_str}\\n\\nYou are tasked with executing step {1}, {task}.\"\"\"\n",
    "    agent_response = await agent_executor.ainvoke(\n",
    "        {\"messages\": [(\"user\", task_formatted)]}\n",
    "    )\n",
    "    return {\n",
    "        \"past_steps\": [(task, agent_response[\"messages\"][-1].content)],\n",
    "    }\n",
    "\n",
    "\n",
    "async def plan_step(state: PlanExecute):\n",
    "    plan = await planner.ainvoke({\"messages\": [(\"user\", state[\"input\"])]})\n",
    "    return {\"plan\": plan.steps}\n",
    "\n",
    "\n",
    "async def replan_step(state: PlanExecute):\n",
    "    output = await replanner.ainvoke(state)\n",
    "    if isinstance(output.action, Response):\n",
    "        return {\"response\": output.action.response}\n",
    "    else:\n",
    "        return {\"plan\": output.action.steps}\n",
    "\n",
    "\n",
    "def should_end(state: PlanExecute):\n",
    "    if \"response\" in state and state[\"response\"]:\n",
    "        return END\n",
    "    else:\n",
    "        return \"agent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cae6b52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "workflow = StateGraph(PlanExecute)\n",
    "\n",
    "# Add the plan node\n",
    "workflow.add_node(\"planner\", plan_step)\n",
    "\n",
    "# Add the execution step\n",
    "workflow.add_node(\"agent\", execute_step)\n",
    "\n",
    "# Add a replan node\n",
    "workflow.add_node(\"replan\", replan_step)\n",
    "\n",
    "workflow.add_edge(START, \"planner\")\n",
    "\n",
    "# From plan we go to agent\n",
    "workflow.add_edge(\"planner\", \"agent\")\n",
    "\n",
    "# From agent, we replan\n",
    "workflow.add_edge(\"agent\", \"replan\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"replan\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_end,\n",
    "    [\"agent\", END],\n",
    ")\n",
    "\n",
    "# Finally, we compile it!\n",
    "# This compiles it into a LangChain Runnable,\n",
    "# meaning you can use it as you would any other runnable\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0e9a3a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(app.get_graph(xray=True).draw_mermaid_png()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "83dc0545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plan': [\"Identify the winner of the men's 2024 Australian Open.\", 'Research the hometown of the identified winner.', 'Provide the hometown as the final answer.']}\n",
      "{'past_steps': [(\"Identify the winner of the men's 2024 Australian Open.\", \"The winner of the men's singles title at the 2024 Australian Open is Jannik Sinner from Italy.\")]}\n"
     ]
    },
    {
     "ename": "APIConnectionError",
     "evalue": "Connection error.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cheekish\\Desktop\\Gen-AI\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cheekish\\Desktop\\Gen-AI\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394\u001b[39m, in \u001b[36mAsyncHTTPTransport.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m     resp = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pool.handle_async_request(req)\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.AsyncIterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cheekish\\Desktop\\Gen-AI\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cheekish\\Desktop\\Gen-AI\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m connection.handle_async_request(\n\u001b[32m    237\u001b[39m         pool_request.request\n\u001b[32m    238\u001b[39m     )\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cheekish\\Desktop\\Gen-AI\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101\u001b[39m, in \u001b[36mAsyncHTTPConnection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_async_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cheekish\\Desktop\\Gen-AI\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78\u001b[39m, in \u001b[36mAsyncHTTPConnection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     stream = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connect(request)\n\u001b[32m     80\u001b[39m     ssl_object = stream.get_extra_info(\u001b[33m\"\u001b[39m\u001b[33mssl_object\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cheekish\\Desktop\\Gen-AI\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:156\u001b[39m, in \u001b[36mAsyncHTTPConnection._connect\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mstart_tls\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     stream = \u001b[38;5;28;01mawait\u001b[39;00m stream.start_tls(**kwargs)\n\u001b[32m    157\u001b[39m     trace.return_value = stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cheekish\\Desktop\\Gen-AI\\.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:67\u001b[39m, in \u001b[36mAnyIOStream.start_tls\u001b[39m\u001b[34m(self, ssl_context, server_hostname, timeout)\u001b[39m\n\u001b[32m     61\u001b[39m exc_map = {\n\u001b[32m     62\u001b[39m     \u001b[38;5;167;01mTimeoutError\u001b[39;00m: ConnectTimeout,\n\u001b[32m     63\u001b[39m     anyio.BrokenResourceError: ConnectError,\n\u001b[32m     64\u001b[39m     anyio.EndOfStream: ConnectError,\n\u001b[32m     65\u001b[39m     ssl.SSLError: ConnectError,\n\u001b[32m     66\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py:155\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m     \u001b[38;5;28mself\u001b[39m.gen.throw(typ, value, traceback)\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    157\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    158\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    159\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cheekish\\Desktop\\Gen-AI\\.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14\u001b[39m, in \u001b[36mmap_exceptions\u001b[39m\u001b[34m(map)\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mConnectError\u001b[39m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:992)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cheekish\\Desktop\\Gen-AI\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1484\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1483\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1484\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.send(\n\u001b[32m   1485\u001b[39m         request,\n\u001b[32m   1486\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_stream_response_body(request=request),\n\u001b[32m   1487\u001b[39m         **kwargs,\n\u001b[32m   1488\u001b[39m     )\n\u001b[32m   1489\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cheekish\\Desktop\\Gen-AI\\.venv\\Lib\\site-packages\\httpx\\_client.py:1629\u001b[39m, in \u001b[36mAsyncClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m   1627\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m-> \u001b[39m\u001b[32m1629\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_auth(\n\u001b[32m   1630\u001b[39m     request,\n\u001b[32m   1631\u001b[39m     auth=auth,\n\u001b[32m   1632\u001b[39m     follow_redirects=follow_redirects,\n\u001b[32m   1633\u001b[39m     history=[],\n\u001b[32m   1634\u001b[39m )\n\u001b[32m   1635\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cheekish\\Desktop\\Gen-AI\\.venv\\Lib\\site-packages\\httpx\\_client.py:1657\u001b[39m, in \u001b[36mAsyncClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m   1656\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1657\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_redirects(\n\u001b[32m   1658\u001b[39m         request,\n\u001b[32m   1659\u001b[39m         follow_redirects=follow_redirects,\n\u001b[32m   1660\u001b[39m         history=history,\n\u001b[32m   1661\u001b[39m     )\n\u001b[32m   1662\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cheekish\\Desktop\\Gen-AI\\.venv\\Lib\\site-packages\\httpx\\_client.py:1694\u001b[39m, in \u001b[36mAsyncClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m   1692\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m hook(request)\n\u001b[32m-> \u001b[39m\u001b[32m1694\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_single_request(request)\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cheekish\\Desktop\\Gen-AI\\.venv\\Lib\\site-packages\\httpx\\_client.py:1730\u001b[39m, in \u001b[36mAsyncClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1729\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1730\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m transport.handle_async_request(request)\n\u001b[32m   1732\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, AsyncByteStream)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cheekish\\Desktop\\Gen-AI\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393\u001b[39m, in \u001b[36mAsyncHTTPTransport.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    381\u001b[39m req = httpcore.Request(\n\u001b[32m    382\u001b[39m     method=request.method,\n\u001b[32m    383\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     extensions=request.extensions,\n\u001b[32m    392\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_httpcore_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mawait\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_async_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py:155\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m     \u001b[38;5;28mself\u001b[39m.gen.throw(typ, value, traceback)\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    157\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    158\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    159\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cheekish\\Desktop\\Gen-AI\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    117\u001b[39m message = \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mConnectError\u001b[39m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:992)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mAPIConnectionError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m config = {\u001b[33m\"\u001b[39m\u001b[33mrecursion_limit\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m50\u001b[39m}\n\u001b[32m      2\u001b[39m inputs = {\u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mwhat is the hometown of the mens 2024 Australia open winner?\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m app.astream(inputs, config=config):\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m event.items():\n\u001b[32m      5\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m k != \u001b[33m\"\u001b[39m\u001b[33m__end__\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cheekish\\Desktop\\Gen-AI\\.venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2740\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2734\u001b[39m \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[32m   2735\u001b[39m \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[32m   2736\u001b[39m \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[32m   2737\u001b[39m \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[32m   2738\u001b[39m \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[32m   2739\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m loop.tick(input_keys=\u001b[38;5;28mself\u001b[39m.input_channels):\n\u001b[32m-> \u001b[39m\u001b[32m2740\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   2741\u001b[39m         loop.tasks.values(),\n\u001b[32m   2742\u001b[39m         timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   2743\u001b[39m         retry_policy=\u001b[38;5;28mself\u001b[39m.retry_policy,\n\u001b[32m   2744\u001b[39m         get_waiter=get_waiter,\n\u001b[32m   2745\u001b[39m     ):\n\u001b[32m   2746\u001b[39m         \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2747\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m output():\n\u001b[32m   2748\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cheekish\\Desktop\\Gen-AI\\.venv\\Lib\\site-packages\\langgraph\\pregel\\runner.py:283\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[39m\n\u001b[32m    281\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[32m    284\u001b[39m         t,\n\u001b[32m    285\u001b[39m         retry_policy,\n\u001b[32m    286\u001b[39m         stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    287\u001b[39m         configurable={\n\u001b[32m    288\u001b[39m             CONFIG_KEY_CALL: partial(\n\u001b[32m    289\u001b[39m                 _acall,\n\u001b[32m    290\u001b[39m                 weakref.ref(t),\n\u001b[32m    291\u001b[39m                 stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    292\u001b[39m                 retry=retry_policy,\n\u001b[32m    293\u001b[39m                 futures=weakref.ref(futures),\n\u001b[32m    294\u001b[39m                 schedule_task=\u001b[38;5;28mself\u001b[39m.schedule_task,\n\u001b[32m    295\u001b[39m                 submit=\u001b[38;5;28mself\u001b[39m.submit,\n\u001b[32m    296\u001b[39m                 reraise=reraise,\n\u001b[32m    297\u001b[39m                 loop=loop,\n\u001b[32m    298\u001b[39m             ),\n\u001b[32m    299\u001b[39m         },\n\u001b[32m    300\u001b[39m     )\n\u001b[32m    301\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cheekish\\Desktop\\Gen-AI\\.venv\\Lib\\site-packages\\langgraph\\pregel\\retry.py:128\u001b[39m, in \u001b[36marun_with_retry\u001b[39m\u001b[34m(task, retry_policies, stream, configurable)\u001b[39m\n\u001b[32m    126\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task.proc.ainvoke(task.input, config)\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    130\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cheekish\\Desktop\\Gen-AI\\.venv\\Lib\\site-packages\\langgraph\\utils\\runnable.py:672\u001b[39m, in \u001b[36mRunnableSeq.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    670\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    671\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m672\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(\n\u001b[32m    673\u001b[39m             step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs), context=context\n\u001b[32m    674\u001b[39m         )\n\u001b[32m    675\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    676\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cheekish\\Desktop\\Gen-AI\\.venv\\Lib\\site-packages\\langgraph\\utils\\runnable.py:440\u001b[39m, in \u001b[36mRunnableCallable.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    438\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_chain_end(ret)\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.afunc(*args, **kwargs)\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    442\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m ret.ainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mreplan_step\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreplan_step\u001b[39m(state: PlanExecute):\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     output = \u001b[38;5;28;01mawait\u001b[39;00m replanner.ainvoke(state)\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output.action, Response):\n\u001b[32m     27\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m: output.action.response}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cheekish\\Desktop\\Gen-AI\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3075\u001b[39m, in \u001b[36mRunnableSequence.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3073\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3074\u001b[39m                 part = functools.partial(step.ainvoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[32m-> \u001b[39m\u001b[32m3075\u001b[39m             \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m coro_with_context(part(), context, create_task=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   3076\u001b[39m     \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3077\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cheekish\\Desktop\\Gen-AI\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5429\u001b[39m, in \u001b[36mRunnableBindingBase.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5422\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5423\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m   5424\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5427\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   5428\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5429\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bound.ainvoke(\n\u001b[32m   5430\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   5431\u001b[39m         \u001b[38;5;28mself\u001b[39m._merge_configs(config),\n\u001b[32m   5432\u001b[39m         **{**\u001b[38;5;28mself\u001b[39m.kwargs, **kwargs},\n\u001b[32m   5433\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cheekish\\Desktop\\Gen-AI\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:391\u001b[39m, in \u001b[36mBaseChatModel.ainvoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    381\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m    383\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    388\u001b[39m     **kwargs: Any,\n\u001b[32m    389\u001b[39m ) -> BaseMessage:\n\u001b[32m    390\u001b[39m     config = ensure_config(config)\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m     llm_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate_prompt(\n\u001b[32m    392\u001b[39m         [\u001b[38;5;28mself\u001b[39m._convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[32m    393\u001b[39m         stop=stop,\n\u001b[32m    394\u001b[39m         callbacks=config.get(\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    395\u001b[39m         tags=config.get(\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    396\u001b[39m         metadata=config.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    397\u001b[39m         run_name=config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    398\u001b[39m         run_id=config.pop(\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    399\u001b[39m         **kwargs,\n\u001b[32m    400\u001b[39m     )\n\u001b[32m    401\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m, llm_result.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cheekish\\Desktop\\Gen-AI\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:957\u001b[39m, in \u001b[36mBaseChatModel.agenerate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    948\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    949\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magenerate_prompt\u001b[39m(\n\u001b[32m    950\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    954\u001b[39m     **kwargs: Any,\n\u001b[32m    955\u001b[39m ) -> LLMResult:\n\u001b[32m    956\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate(\n\u001b[32m    958\u001b[39m         prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n\u001b[32m    959\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cheekish\\Desktop\\Gen-AI\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:915\u001b[39m, in \u001b[36mBaseChatModel.agenerate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[32m    903\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    904\u001b[39m             *[\n\u001b[32m    905\u001b[39m                 run_manager.on_llm_end(\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m             ]\n\u001b[32m    914\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[32m0\u001b[39m]\n\u001b[32m    916\u001b[39m flattened_outputs = [\n\u001b[32m    917\u001b[39m     LLMResult(generations=[res.generations], llm_output=res.llm_output)  \u001b[38;5;66;03m# type: ignore[list-item, union-attr]\u001b[39;00m\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[32m    919\u001b[39m ]\n\u001b[32m    920\u001b[39m llm_output = \u001b[38;5;28mself\u001b[39m._combine_llm_outputs([res.llm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cheekish\\Desktop\\Gen-AI\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1083\u001b[39m, in \u001b[36mBaseChatModel._agenerate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1081\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1082\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._agenerate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1083\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(\n\u001b[32m   1084\u001b[39m         messages, stop=stop, run_manager=run_manager, **kwargs\n\u001b[32m   1085\u001b[39m     )\n\u001b[32m   1086\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1087\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cheekish\\Desktop\\Gen-AI\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1165\u001b[39m, in \u001b[36mBaseChatOpenAI._agenerate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1163\u001b[39m payload.pop(\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1164\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1165\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.root_async_client.beta.chat.completions.parse(\n\u001b[32m   1166\u001b[39m         **payload\n\u001b[32m   1167\u001b[39m     )\n\u001b[32m   1168\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m openai.BadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1169\u001b[39m     _handle_openai_bad_request(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cheekish\\Desktop\\Gen-AI\\.venv\\Lib\\site-packages\\openai\\resources\\beta\\chat\\completions.py:437\u001b[39m, in \u001b[36mAsyncCompletions.parse\u001b[39m\u001b[34m(self, messages, model, audio, response_format, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, seed, service_tier, stop, store, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    430\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparser\u001b[39m(raw_completion: ChatCompletion) -> ParsedChatCompletion[ResponseFormatT]:\n\u001b[32m    431\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _parse_chat_completion(\n\u001b[32m    432\u001b[39m         response_format=response_format,\n\u001b[32m    433\u001b[39m         chat_completion=raw_completion,\n\u001b[32m    434\u001b[39m         input_tools=tools,\n\u001b[32m    435\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m    438\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    439\u001b[39m     body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m    440\u001b[39m         {\n\u001b[32m    441\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m    442\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m    443\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m    444\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m    445\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m    446\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m    447\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m    448\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m    449\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m    450\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m    451\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m    452\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m    453\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m    454\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m    455\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m    456\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m    457\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m    458\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: _type_to_response_format(response_format),\n\u001b[32m    459\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m    460\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m    461\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m    462\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m    463\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    464\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m    465\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m    466\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m    467\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m    468\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m    469\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m    470\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m    471\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m    472\u001b[39m         },\n\u001b[32m    473\u001b[39m         completion_create_params.CompletionCreateParams,\n\u001b[32m    474\u001b[39m     ),\n\u001b[32m    475\u001b[39m     options=make_request_options(\n\u001b[32m    476\u001b[39m         extra_headers=extra_headers,\n\u001b[32m    477\u001b[39m         extra_query=extra_query,\n\u001b[32m    478\u001b[39m         extra_body=extra_body,\n\u001b[32m    479\u001b[39m         timeout=timeout,\n\u001b[32m    480\u001b[39m         post_parser=parser,\n\u001b[32m    481\u001b[39m     ),\n\u001b[32m    482\u001b[39m     \u001b[38;5;66;03m# we turn the `ChatCompletion` instance into a `ParsedChatCompletion`\u001b[39;00m\n\u001b[32m    483\u001b[39m     \u001b[38;5;66;03m# in the `parser` function above\u001b[39;00m\n\u001b[32m    484\u001b[39m     cast_to=cast(Type[ParsedChatCompletion[ResponseFormatT]], ChatCompletion),\n\u001b[32m    485\u001b[39m     stream=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    486\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cheekish\\Desktop\\Gen-AI\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1742\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1728\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1729\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1730\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1737\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1738\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1739\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1740\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1741\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1742\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cheekish\\Desktop\\Gen-AI\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1516\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1513\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1515\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRaising connection error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1516\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request=request) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   1518\u001b[39m log.debug(\n\u001b[32m   1519\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mHTTP Response: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m,\n\u001b[32m   1520\u001b[39m     request.method,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1524\u001b[39m     response.headers,\n\u001b[32m   1525\u001b[39m )\n\u001b[32m   1526\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mrequest_id: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, response.headers.get(\u001b[33m\"\u001b[39m\u001b[33mx-request-id\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mAPIConnectionError\u001b[39m: Connection error.",
      "During task with name 'replan' and id 'b00fe21e-b5d4-0409-8aeb-1ee2a33d80a5'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "config = {\"recursion_limit\": 50}\n",
    "inputs = {\"input\": \"what is the hometown of the mens 2024 Australia open winner?\"}\n",
    "async for event in app.astream(inputs, config=config):\n",
    "    for k, v in event.items():\n",
    "        if k != \"__end__\":\n",
    "            print(v)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78bfdd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
